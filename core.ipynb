{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "#config for colab not ....\n",
    "SOURCE_DATA_DIR = \"/content/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\n",
    "PREPROCESSED_DIR = \"/content/preprocessed_data\"\n",
    "IMG_SIZE = 192\n",
    "\n",
    "# once preproccess for faster train\n",
    "def preprocess_and_save():\n",
    "    os.makedirs(PREPROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "    print(\"Starting one-time preprocessing of the dataset on Colab storage...\")\n",
    "\n",
    "    samples_to_process = []\n",
    "    for s in sorted(os.listdir(SOURCE_DATA_DIR)):\n",
    "        try:\n",
    "            num = int(s.split(\"_\")[-1])\n",
    "            if num > 354: continue\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        base_path = os.path.join(SOURCE_DATA_DIR, s)\n",
    "        required = [f\"{s}_flair.nii\", f\"{s}_t1.nii\", f\"{s}_t1ce.nii\", f\"{s}_t2.nii\", f\"{s}_seg.nii\"]\n",
    "        if all(os.path.exists(os.path.join(base_path, f)) for f in required):\n",
    "            seg_path = os.path.join(base_path, f\"{s}_seg.nii\")\n",
    "            seg_volume = nib.load(seg_path).get_fdata()\n",
    "            for slice_idx in range(seg_volume.shape[2]):\n",
    "                if np.any(seg_volume[:, :, slice_idx] > 0):\n",
    "                    samples_to_process.append((s, slice_idx))\n",
    "\n",
    "    print(f\"Found {len(samples_to_process)} slices. Now processing and saving them as .npy files...\")\n",
    "\n",
    "    for i, (sample_id, slice_idx) in enumerate(tqdm(samples_to_process, desc=\"Preprocessing Slices\")):\n",
    "        sample_path = os.path.join(SOURCE_DATA_DIR, sample_id)\n",
    "\n",
    "        flair = nib.load(os.path.join(sample_path, sample_id + \"_flair.nii\")).get_fdata()[:, :, slice_idx]\n",
    "        t1 = nib.load(os.path.join(sample_path, sample_id + \"_t1.nii\")).get_fdata()[:, :, slice_idx]\n",
    "        t1ce = nib.load(os.path.join(sample_path, sample_id + \"_t1ce.nii\")).get_fdata()[:, :, slice_idx]\n",
    "        t2 = nib.load(os.path.join(sample_path, sample_id + \"_t2.nii\")).get_fdata()[:, :, slice_idx]\n",
    "        mask = nib.load(os.path.join(sample_path, sample_id + \"_seg.nii\")).get_fdata()[:, :, slice_idx].astype(np.int64)\n",
    "\n",
    "        image_stack = np.stack([flair, t1, t1ce, t2], axis=-1)\n",
    "        image_resized = cv2.resize(image_stack, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_LINEAR)\n",
    "        mask_resized = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        image_processed = image_resized.transpose(2, 0, 1)\n",
    "        image_processed = (image_processed - np.mean(image_processed)) / (np.std(image_processed) + 1e-8)\n",
    "\n",
    "        mask_processed = mask_resized\n",
    "        mask_processed[mask_processed == 4] = 3\n",
    "\n",
    "        binary_mask = (mask_processed > 0).astype(np.uint8)\n",
    "        boundary_map = np.zeros(mask_processed.shape, dtype=np.float32)\n",
    "        contours, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "        cv2.drawContours(boundary_map, contours, -1, (1.0), thickness=2)\n",
    "\n",
    "        filename = f\"slice_{i:05d}.npy\"\n",
    "        filepath = os.path.join(PREPROCESSED_DIR, filename)\n",
    "        np.save(filepath, {'image': image_processed, 'mask': mask_processed, 'boundary': boundary_map})\n",
    "\n",
    "    print(\"Preprocessing complete! Data is ready in /content/preprocessed_data/\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    preprocess_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "PREPROCESSED_DIR = \"/content/preprocessed_data\"\n",
    "CLASS_COUNT = 4\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "class BraTSDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.filepaths = sorted([os.path.join(root_dir, f) for f in os.listdir(root_dir) if f.endswith('.npy')])\n",
    "        print(f\"Found {len(self.filepaths)} preprocessed slices in {root_dir}.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filepath = self.filepaths[idx]\n",
    "        data = np.load(filepath, allow_pickle=True).item()\n",
    "        return (\n",
    "            torch.tensor(data['image'], dtype=torch.float32),\n",
    "            torch.tensor(data['mask'], dtype=torch.long),\n",
    "            torch.tensor(data['boundary'], dtype=torch.float32).unsqueeze(0)\n",
    "        )\n",
    "\n",
    "# --- Boundary-Aware Loss Function ---\n",
    "dice_loss = smp.losses.DiceLoss(mode='multiclass', from_logits=True)\n",
    "focal_loss = smp.losses.FocalLoss(mode='multiclass')\n",
    "boundary_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def boundary_aware_loss(pred, target_mask, target_boundary):\n",
    "    seg_loss = dice_loss(pred, target_mask) + focal_loss(pred, target_mask)\n",
    "    pred_softmax = F.softmax(pred, dim=1)\n",
    "    pred_for_boundary = 1 - pred_softmax[:, 0, :, :].unsqueeze(1)\n",
    "    b_loss = boundary_loss(pred_for_boundary, target_boundary)\n",
    "    combined_loss = seg_loss + (2.0 * b_loss)\n",
    "    return combined_loss\n",
    "#train\n",
    "def train(model, loader, optimizer, scheduler, scaler):\n",
    "    loss_history = []\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=True)\n",
    "        epoch_loss = 0.0\n",
    "        for images, masks, boundaries in progress_bar:\n",
    "            images, masks, boundaries = images.to(DEVICE), masks.to(DEVICE), boundaries.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = boundary_aware_loss(outputs, masks, boundaries)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "        avg_loss = epoch_loss / len(loader)\n",
    "        loss_history.append(avg_loss)\n",
    "        scheduler.step()\n",
    "    return loss_history\n",
    "\n",
    "#visualization\n",
    "def visualize_results(model, dataset, count=10):\n",
    "    fig, axs = plt.subplots(count, 3, figsize=(15, count * 5))\n",
    "    fig.suptitle(\"Model Predictions vs. Ground Truth\", fontsize=20)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(count):\n",
    "            idx = np.random.randint(0, len(dataset))\n",
    "            image, mask, _ = dataset[idx]\n",
    "            image_for_pred = image.unsqueeze(0).to(DEVICE)\n",
    "            pred = model(image_for_pred)\n",
    "            pred_mask = torch.argmax(pred.squeeze(0), dim=0).cpu().numpy()\n",
    "            display_image = image[0, :, :].numpy()\n",
    "            true_mask_color = colorize_mask(mask.numpy())\n",
    "            pred_mask_color = colorize_mask(pred_mask)\n",
    "            axs[i, 0].imshow(display_image, cmap='bone')\n",
    "            axs[i, 0].set_title(f\"Input Image (Flair) - Sample {idx}\")\n",
    "            axs[i, 0].axis('off')\n",
    "            axs[i, 1].imshow(true_mask_color)\n",
    "            axs[i, 1].set_title(\"Ground Truth Mask\")\n",
    "            axs[i, 1].axis('off')\n",
    "            axs[i, 2].imshow(pred_mask_color)\n",
    "            axs[i, 2].set_title(\"Predicted Mask\")\n",
    "            axs[i, 2].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def colorize_mask(mask):\n",
    "    colors = {0: (0, 0, 0), 1: (0, 255, 0), 2: (255, 255, 0), 3: (255, 0, 0)}\n",
    "    color_mask = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "    for label, color in colors.items():\n",
    "        color_mask[mask == label] = color\n",
    "    return color_mask\n",
    "\n",
    "#main exec\n",
    "if __name__ == '__main__':\n",
    "    full_dataset = BraTSDataset(root_dir=PREPROCESSED_DIR)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    model = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", in_channels=4, classes=CLASS_COUNT).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-6)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    print(\"Starting training from preprocessed data on Colab storage...\")\n",
    "    loss_history = train(model, train_loader, optimizer, scheduler, scaler)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(loss_history, label='Training Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Visualizing results on validation data...\")\n",
    "    visualize_results(model, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Found 23415 preprocessed slices in /content/preprocessed_data.\n",
    "/tmp/ipython-input-758465914.py:124: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
    "  scaler = GradScaler()\n",
    "Starting training from preprocessed data on Colab storage...\n",
    "Epoch 1/25:   0%|          | 0/1171 [00:00<?, ?it/s]/tmp/ipython-input-758465914.py:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
    "  with autocast():\n",
    "Epoch 1/25: 100%|██████████| 1171/1171 [02:30<00:00,  7.78it/s, loss=1.7039]\n",
    "Epoch 2/25: 100%|██████████| 1171/1171 [02:21<00:00,  8.27it/s, loss=1.5865]\n",
    "Epoch 3/25: 100%|██████████| 1171/1171 [02:17<00:00,  8.51it/s, loss=1.5173]\n",
    "Epoch 4/25: 100%|██████████| 1171/1171 [02:17<00:00,  8.51it/s, loss=1.5119]\n",
    "Epoch 5/25: 100%|██████████| 1171/1171 [02:18<00:00,  8.48it/s, loss=1.5457]\n",
    "Epoch 6/25: 100%|██████████| 1171/1171 [02:16<00:00,  8.55it/s, loss=1.5448]\n",
    "Epoch 7/25: 100%|██████████| 1171/1171 [02:16<00:00,  8.56it/s, loss=1.5214]\n",
    "Epoch 8/25: 100%|██████████| 1171/1171 [02:16<00:00,  8.60it/s, loss=1.5079]\n",
    "Epoch 9/25: 100%|██████████| 1171/1171 [02:15<00:00,  8.62it/s, loss=1.5005]\n",
    "Epoch 10/25: 100%|██████████| 1171/1171 [02:16<00:00,  8.60it/s, loss=1.4944]\n",
    "Epoch 11/25: 100%|██████████| 1171/1171 [02:15<00:00,  8.62it/s, loss=1.5095]\n",
    "Epoch 12/25: 100%|██████████| 1171/1171 [02:15<00:00,  8.64it/s, loss=1.4912]\n",
    "Epoch 13/25: 100%|██████████| 1171/1171 [02:15<00:00,  8.61it/s, loss=1.4735]\n",
    "Epoch 14/25: 100%|██████████| 1171/1171 [02:15<00:00,  8.66it/s, loss=1.4791]\n",
    "Epoch 15/25: 100%|██████████| 1171/1171 [02:16<00:00,  8.57it/s, loss=1.4743]\n",
    "Epoch 16/25: 100%|██████████| 1171/1171 [02:17<00:00,  8.50it/s, loss=1.4796]\n",
    "Epoch 17/25: 100%|██████████| 1171/1171 [02:16<00:00,  8.58it/s, loss=1.4748]\n",
    "Epoch 18/25: 100%|██████████| 1171/1171 [02:18<00:00,  8.47it/s, loss=1.4675]\n",
    "Epoch 19/25: 100%|██████████| 1171/1171 [02:17<00:00,  8.54it/s, loss=1.4642]\n",
    "Epoch 20/25: 100%|██████████| 1171/1171 [02:17<00:00,  8.54it/s, loss=1.4661]\n",
    "Epoch 21/25: 100%|██████████| 1171/1171 [02:18<00:00,  8.46it/s, loss=1.4689]\n",
    "Epoch 22/25: 100%|██████████| 1171/1171 [02:17<00:00,  8.49it/s, loss=1.4637]\n",
    "Epoch 23/25: 100%|██████████| 1171/1171 [02:17<00:00,  8.51it/s, loss=1.4828]\n",
    "Epoch 24/25: 100%|██████████| 1171/1171 [02:17<00:00,  8.51it/s, loss=1.4907]\n",
    "Epoch 25/25: 100%|██████████| 1171/1171 [02:18<00:00,  8.46it/s, loss=1.4725]\n",
    "\n",
    "Visualizing results on validation data...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
